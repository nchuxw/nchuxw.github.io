---
title: 'High-Performance Large-Scale Image Recognition Without Normalization'
date: 2021-02-22 09:35:00
categories: 论文解读
tags: [论文, 指纹识别]
mathjax: true
---

[论文原文下载链接](https://cdn.jsdelivr.net/gh/stxw/stxw.github.io/documnets/papers/High-Performance_Large-Scale_Image_Recognition_Without_Normalization.pdf)


# Abstract（摘要）
&emsp;&emsp;批量归一化是大多数图像分类模型的关键组成部分，但由于其对批处理大小的依赖和实例之间的交互，它有许多不需要的特性。虽然最近的工作已经成功地训练了没有归一化层的深度重网络，但这些模型无法与最好的批归一化网络的测试精度竞争，而且对于大学习率或强数据增强通常不稳定。在这项工作中，我们开发了一种自适应梯度裁剪技术来克服这些不稳定性，并设计了一种显著改进的Normalizer-Free ResNets类。我们较小模型EfficientNet-B7的匹配测试准确率，在ImageNet上训练可以快8.7倍，我们最大的模型达到了新的最先进top-1准确率86.5%。此外，在对3亿张标记图像数据集进行大规模的预训练后，在ImageNet上进行微调时，无归一化模型获得了比批量归一化模型显著更好的性能，我们最好的模型获得了89.2%的准确率。


# Introduction（介绍）
&emsp;&emsp;最近计算机视觉中的绝大多数模型都是深度残差网络的变体，训练时经过了批量归一化处理。这两种架构创新的结合使从业者能够训练更深的网络，从而在训练集和测试集上获得更高的准确性。批归一化还可以平滑损失情况，这使得更大学习率和更大批量大小能稳定训练，而且它还有正则化的效果。但是，批处理规范化有三个显著的实际缺点：
1. 首先，它是一个非常昂贵的计算原语，这会导致内存开销，并显著增加了在一些网络中评估梯度所需的时间。
2. 其次，它引入了模型在训练期间和推断时之间的行为差异，引入了需要调整的隐藏超参数。
3. 第三，也是最重要的，批量归一化打破了小批量处理中训练样本之间的独立性。

&emsp;&emsp;这三个特性有一系列的负面影响。例如，实践者发现批量归一化网络通常很难在不同的硬件上精确复制，而且批量归一化往往会导致微小的实现错误，特别是在分布式训练中。此外，批量归一化不能用于某些任务，因为批量中的训练样本之间的交互使网络能够“欺骗”某些损失函数。例如，批量归一化在一些对比学习算法中需要特别注意防止信息泄露。这也是序列建模任务的主要关注点，这促使语言模型采用替代归一化的方法。如果在训练过程中批量统计有较大的方差，则批量归一化网络的性能也会降低。最后，批量归一化的性能对批量大小很敏感，批量大小过小时批量归一化网络性能较差，这限制了我们在有限硬件上训练的最大模型尺寸。我们在附录B中扩展了与批量归一化相关的挑战。

&emsp;&emsp;因此，尽管批处理归一化使深度学习社区在最近几年取得了长足的进步，但我们预计，从长远来看，它可能会阻碍进步。我们相信社区应该寻找一种简单的替代方法，它可以实现具有竞争力的测试准确性，并可以用于广泛的任务。



